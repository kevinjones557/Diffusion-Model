{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import imageio\n",
    "import time\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import *\n",
    "from data import *\n",
    "\n",
    "# Initialize the model\n",
    "t_range = 100  # Number of steps\n",
    "image_size = (1, 3, 32, 32)  # Example input image size (batch, channels, height, width)\n",
    "img_depth = 3  # Number of channels in the image\n",
    "dataset_choice = \"Cifar-10\"\n",
    "batch_size=256\n",
    "device = 'cuda'\n",
    "# Example image (normalized between 0 and 1)\n",
    "\n",
    "train_loader = get_dataloader(dataset_name=dataset_choice, batch_size=batch_size)\n",
    "validation_loader = get_dataloader(dataset_name=dataset_choice, batch_size=batch_size, split='validation')\n",
    "\n",
    "model = DiffusionModel(in_size=32 * 32, t_range=t_range, img_depth=img_depth, device=device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "num_epochs = 5  # Number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12920323\n"
     ]
    }
   ],
   "source": [
    "print(sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'../model_{t_range}_float16_precision.pth'))\n",
    "\n",
    "gif_shape = [8, 8]\n",
    "sample_batch_size = gif_shape[0] * gif_shape[1]\n",
    "n_hold_final = 10\n",
    "\n",
    "# Generate samples from denoising process\n",
    "gen_samples = []\n",
    "x = torch.randn((sample_batch_size, img_depth, 32, 32)).to(device)\n",
    "sample_steps = torch.arange(model.t_range-1, 0, -1).to(device)\n",
    "for t in sample_steps:\n",
    "    with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "        x = model.denoise_sample(x, t)\n",
    "    if t % 50 == 0:\n",
    "        gen_samples.append(x)\n",
    "for _ in range(n_hold_final):\n",
    "    gen_samples.append(x)\n",
    "gen_samples = torch.stack(gen_samples, dim=0).moveaxis(2, 4).squeeze(-1)\n",
    "gen_samples = (gen_samples.clamp(-1, 1) + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process samples and save as gif\n",
    "gen_samples = gen_samples.cpu()\n",
    "gen_samples = (gen_samples * 255).type(torch.uint8)\n",
    "gen_samples = gen_samples.reshape(-1, gif_shape[0], gif_shape[1], 32, 32, img_depth)\n",
    "\n",
    "def stack_samples(gen_samples, stack_dim):\n",
    "    gen_samples = list(torch.split(gen_samples, 1, dim=1))\n",
    "    for i in range(len(gen_samples)):\n",
    "        gen_samples[i] = gen_samples[i].squeeze(1)\n",
    "    return torch.cat(gen_samples, dim=stack_dim)\n",
    "\n",
    "gen_samples = stack_samples(gen_samples, 2)\n",
    "gen_samples = stack_samples(gen_samples, 2)\n",
    "\n",
    "imageio.mimsave(\n",
    "    f\"pred_{t_range}_float16.gif\",\n",
    "    list(gen_samples),\n",
    "    fps=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process samples and save as gif\n",
    "gen_samples = gen_samples.cpu()\n",
    "gen_samples = (gen_samples * 255).type(torch.uint8)\n",
    "gen_samples = gen_samples.reshape(-1, gif_shape[0], gif_shape[1], 32, 32, img_depth)\n",
    "\n",
    "def stack_samples(gen_samples, stack_dim):\n",
    "    gen_samples = list(torch.split(gen_samples, 1, dim=1))\n",
    "    for i in range(len(gen_samples)):\n",
    "        gen_samples[i] = gen_samples[i].squeeze(1)\n",
    "    return torch.cat(gen_samples, dim=stack_dim)\n",
    "\n",
    "gen_samples = stack_samples(gen_samples, 2)\n",
    "gen_samples = stack_samples(gen_samples, 2)\n",
    "\n",
    "imageio.mimsave(\n",
    "    f\"pred_{t_range}_float16.gif\",\n",
    "    list(gen_samples),\n",
    "    fps=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kpjon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      " 48%|████▊     | 19/40 [00:24<00:27,  1.32s/it]\n",
      "100%|██████████| 20/20 [52:06<00:00, 156.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 3126.3549897670746\n",
      "FID Score: 25.88897705078125\n",
      "inception_score=(tensor(7.5745, device='cuda:0'), tensor(0.3724, device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "fid = FrechetInceptionDistance(feature=2048).to(device)  # or feature=64 based on the choice\n",
    "inception = InceptionScore().to(device)\n",
    "model.load_state_dict(torch.load(f'../model_{t_range}_bfloat16_precision.pth'))\n",
    "\n",
    "# Update FID metric with all real training images\n",
    "iteration = 1\n",
    "for batch, _ in tqdm(validation_loader):  # Loop through the entire training dataset\n",
    "    real_images = inverse_transform(batch).byte().to(device)\n",
    "    fid.update(real_images, real=True)\n",
    "    if iteration == 20:\n",
    "        break\n",
    "    iteration +=1 \n",
    "\n",
    "# Generate the same number of images as the total training dataset\n",
    "generated_images = []\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(20)):\n",
    "        noise = torch.randn(batch_size, 3, 32, 32).to(device)  # Start with noise\n",
    "        gen_images = noise\n",
    "        sample_steps = torch.arange(model.t_range-1, 0, -1).to(device)\n",
    "        for t in sample_steps:\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                gen_images = model.denoise_sample(gen_images, torch.tensor([t]).to(device))\n",
    "        gen_images = inverse_transform(gen_images).byte()\n",
    "        fid.update(gen_images, real=False)\n",
    "        inception.update(gen_images)\n",
    "end = time.time()\n",
    "print(f\"Time elapsed {end - start}\")\n",
    "\n",
    "# Compute the FID score\n",
    "fid_score = fid.compute()\n",
    "print(f\"FID Score: {fid_score}\")\n",
    "inception_score = inception.compute()\n",
    "print(f\"{inception_score=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(generated_images))\n",
    "generated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "img, _ = next(iter(train_loader))\n",
    "img = img[0].squeeze()\n",
    "img = inverse_transform(img).byte()\n",
    "print(img.dtype)\n",
    "print(img.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    noise = torch.randn(1, 3, 32, 32).to(device)  # Start with noise\n",
    "    gen_images = noise\n",
    "    sample_steps = torch.arange(t_range-1, 0, -1).to(device)\n",
    "    for t in sample_steps:\n",
    "        gen_images = model.denoise_sample(gen_images, torch.tensor([t]).to(device))\n",
    "    img = gen_images.squeeze(0).cpu()\n",
    "    img = inverse_transform(img).byte()\n",
    "    plt.imshow(img.permute(1,2,0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
